{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Descent2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBfU7bBsrGe-"
      },
      "source": [
        "vectorized gradient descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn6QMbflrDHX",
        "outputId": "d14b5f8c-7b83-41d2-ede0-9bfc6c3906d8"
      },
      "source": [
        "import numpy as np                                    \n",
        "from sklearn import datasets                                                        \n",
        "from sklearn.linear_model import LinearRegression     \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing      \n",
        "import statsmodels.api as sm\n",
        "\n",
        "boston = datasets.load_boston()          \n",
        "X = boston.data                          \n",
        "Y = boston.target  \n",
        "M, N = X.shape\n",
        "\n",
        "# standardization :\n",
        "std_scaler = preprocessing.StandardScaler()\n",
        "standardized_X = std_scaler.fit_transform(X)\n",
        "allOnes = np.ones((len(standardized_X), 1))\n",
        "standardized_X2 = np.hstack([allOnes, standardized_X])\n",
        "\n",
        "#print(score(standardized_X2,Y,beta))\n",
        "\n",
        "\n",
        "#cost function calculator\n",
        "def cost(X, Y, beta):\n",
        "  return ((Y - (X @ beta))**2).mean()\n",
        "# predictor function\n",
        "def predict(X, beta):\n",
        "  return X @ beta\n",
        "\n",
        "# A function that finds the R^2 Statistic).\n",
        "def score(X, Y, beta):\n",
        "  Y_predicted = predict(X, beta)\n",
        "  u = ((Y - Y_predicted)**2).sum()\n",
        "  v = ((Y - Y.mean())**2).sum()\n",
        "  return 1 - (u/v)\n",
        "\n",
        "def adjScore(X, Y, beta,M,N):\n",
        "  return 1-(1-score(X,Y,beta)*(M-1)/(M-N-1))\n",
        "\n",
        "def Fval(X,Y,beta,M,N):\n",
        "  Y_predicted = predict(X, beta)\n",
        "  u = ((Y - Y_predicted)**2).sum()\n",
        "  v = ((Y - Y.mean())**2).sum()\n",
        "  return ((v-u)/(N-1))/(u/(M-N))\n",
        "\n",
        "# the Batch Gradient Descent function.\n",
        "def GradientDescent(X, Y, alpha, num_iterations, print_cost = False):\n",
        "  M, N = X.shape\n",
        "  X2 = X.copy()\n",
        "  allOnes = np.ones((len(Y), 1))               \n",
        "  X2 = np.hstack([allOnes, X2]) # Concatenating the allOnes column to X2(for the intercept value).\n",
        "  #np.random.seed(0)\n",
        "  beta = np.random.uniform(-10.0, 10.0, N + 1)\n",
        "  cost_array = []\n",
        "\n",
        "  for x in range(num_iterations):\n",
        "    cost_ = cost(X2, Y, beta)\n",
        "    cost_array.append(cost_)\n",
        "    if print_cost:\n",
        "      print(\"Iteration :\", x + 1, '\\t', \"Cost : \" + '%.7f'%cost_)\n",
        "\n",
        "    beta -= (alpha * (2/M) * (X2.T @ (X2@beta - Y)))\n",
        "\n",
        "  return beta, cost_array\n",
        "\n",
        "# the normal equation function\n",
        "def normalEqn(X, y):  \n",
        "    beta = np.linalg.inv((X.T).dot(X)).dot(X.T).dot(y) \n",
        "    return beta # returns array of predictors  \n",
        "\n",
        "beta, cost_array = GradientDescent(standardized_X, Y, 0.1, 250, True)\n",
        "\n",
        "print(\"R^2:\" ,score(standardized_X2,Y,beta))\n",
        "print(\"Adjusted R^2 :\" , adjScore(standardized_X2,Y,beta,M,N))\n",
        "print(\"F-statistic: \" , Fval(standardized_X2,Y,beta,M,N))\n",
        "\n",
        "beta = normalEqn(standardized_X2,Y)\n",
        "print()\n",
        "print(\"using normal equation: \")\n",
        "print()\n",
        "print(\"Cost:\" , cost(standardized_X2,Y,beta))\n",
        "\n",
        "print(\"R^2:\" ,score(standardized_X2,Y,beta))\n",
        "\n",
        "print(\"Adjusted R^2 :\" , adjScore(standardized_X2,Y,beta,M,N))\n",
        "print(\"F-statistic: \" , Fval(standardized_X2,Y,beta,M,N))\n",
        "\n",
        "regr = LinearRegression()\n",
        "regr.fit(X, Y)\n",
        "print(\"scikit training model score: \" , regr.score(X, Y))\n",
        "\n",
        "model = sm.OLS(Y, X)\n",
        "results = model.fit()\n",
        "print(results.summary())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration : 1 \t Cost : 932.1864985\n",
            "Iteration : 2 \t Cost : 492.2250966\n",
            "Iteration : 3 \t Cost : 326.2411116\n",
            "Iteration : 4 \t Cost : 226.5367644\n",
            "Iteration : 5 \t Cost : 162.8086387\n",
            "Iteration : 6 \t Cost : 121.3788862\n",
            "Iteration : 7 \t Cost : 94.0400955\n",
            "Iteration : 8 \t Cost : 75.6933526\n",
            "Iteration : 9 \t Cost : 63.1416290\n",
            "Iteration : 10 \t Cost : 54.3659960\n",
            "Iteration : 11 \t Cost : 48.0822003\n",
            "Iteration : 12 \t Cost : 43.4669379\n",
            "Iteration : 13 \t Cost : 39.9878529\n",
            "Iteration : 14 \t Cost : 37.2973596\n",
            "Iteration : 15 \t Cost : 35.1659674\n",
            "Iteration : 16 \t Cost : 33.4401911\n",
            "Iteration : 17 \t Cost : 32.0158399\n",
            "Iteration : 18 \t Cost : 30.8209738\n",
            "Iteration : 19 \t Cost : 29.8049662\n",
            "Iteration : 20 \t Cost : 28.9314396\n",
            "Iteration : 21 \t Cost : 28.1736718\n",
            "Iteration : 22 \t Cost : 27.5115840\n",
            "Iteration : 23 \t Cost : 26.9297476\n",
            "Iteration : 24 \t Cost : 26.4160516\n",
            "Iteration : 25 \t Cost : 25.9608004\n",
            "Iteration : 26 \t Cost : 25.5560936\n",
            "Iteration : 27 \t Cost : 25.1953948\n",
            "Iteration : 28 \t Cost : 24.8732240\n",
            "Iteration : 29 \t Cost : 24.5849362\n",
            "Iteration : 30 \t Cost : 24.3265577\n",
            "Iteration : 31 \t Cost : 24.0946632\n",
            "Iteration : 32 \t Cost : 23.8862813\n",
            "Iteration : 33 \t Cost : 23.6988210\n",
            "Iteration : 34 \t Cost : 23.5300130\n",
            "Iteration : 35 \t Cost : 23.3778623\n",
            "Iteration : 36 \t Cost : 23.2406099\n",
            "Iteration : 37 \t Cost : 23.1167004\n",
            "Iteration : 38 \t Cost : 23.0047552\n",
            "Iteration : 39 \t Cost : 22.9035502\n",
            "Iteration : 40 \t Cost : 22.8119962\n",
            "Iteration : 41 \t Cost : 22.7291228\n",
            "Iteration : 42 \t Cost : 22.6540637\n",
            "Iteration : 43 \t Cost : 22.5860449\n",
            "Iteration : 44 \t Cost : 22.5243738\n",
            "Iteration : 45 \t Cost : 22.4684303\n",
            "Iteration : 46 \t Cost : 22.4176579\n",
            "Iteration : 47 \t Cost : 22.3715576\n",
            "Iteration : 48 \t Cost : 22.3296807\n",
            "Iteration : 49 \t Cost : 22.2916242\n",
            "Iteration : 50 \t Cost : 22.2570250\n",
            "Iteration : 51 \t Cost : 22.2255566\n",
            "Iteration : 52 \t Cost : 22.1969245\n",
            "Iteration : 53 \t Cost : 22.1708632\n",
            "Iteration : 54 \t Cost : 22.1471331\n",
            "Iteration : 55 \t Cost : 22.1255180\n",
            "Iteration : 56 \t Cost : 22.1058223\n",
            "Iteration : 57 \t Cost : 22.0878696\n",
            "Iteration : 58 \t Cost : 22.0715001\n",
            "Iteration : 59 \t Cost : 22.0565692\n",
            "Iteration : 60 \t Cost : 22.0429462\n",
            "Iteration : 61 \t Cost : 22.0305124\n",
            "Iteration : 62 \t Cost : 22.0191607\n",
            "Iteration : 63 \t Cost : 22.0087936\n",
            "Iteration : 64 \t Cost : 21.9993228\n",
            "Iteration : 65 \t Cost : 21.9906684\n",
            "Iteration : 66 \t Cost : 21.9827576\n",
            "Iteration : 67 \t Cost : 21.9755243\n",
            "Iteration : 68 \t Cost : 21.9689088\n",
            "Iteration : 69 \t Cost : 21.9628564\n",
            "Iteration : 70 \t Cost : 21.9573178\n",
            "Iteration : 71 \t Cost : 21.9522479\n",
            "Iteration : 72 \t Cost : 21.9476057\n",
            "Iteration : 73 \t Cost : 21.9433540\n",
            "Iteration : 74 \t Cost : 21.9394588\n",
            "Iteration : 75 \t Cost : 21.9358894\n",
            "Iteration : 76 \t Cost : 21.9326176\n",
            "Iteration : 77 \t Cost : 21.9296177\n",
            "Iteration : 78 \t Cost : 21.9268665\n",
            "Iteration : 79 \t Cost : 21.9243427\n",
            "Iteration : 80 \t Cost : 21.9220267\n",
            "Iteration : 81 \t Cost : 21.9199011\n",
            "Iteration : 82 \t Cost : 21.9179495\n",
            "Iteration : 83 \t Cost : 21.9161573\n",
            "Iteration : 84 \t Cost : 21.9145111\n",
            "Iteration : 85 \t Cost : 21.9129984\n",
            "Iteration : 86 \t Cost : 21.9116082\n",
            "Iteration : 87 \t Cost : 21.9103301\n",
            "Iteration : 88 \t Cost : 21.9091549\n",
            "Iteration : 89 \t Cost : 21.9080739\n",
            "Iteration : 90 \t Cost : 21.9070793\n",
            "Iteration : 91 \t Cost : 21.9061639\n",
            "Iteration : 92 \t Cost : 21.9053213\n",
            "Iteration : 93 \t Cost : 21.9045453\n",
            "Iteration : 94 \t Cost : 21.9038307\n",
            "Iteration : 95 \t Cost : 21.9031722\n",
            "Iteration : 96 \t Cost : 21.9025653\n",
            "Iteration : 97 \t Cost : 21.9020059\n",
            "Iteration : 98 \t Cost : 21.9014900\n",
            "Iteration : 99 \t Cost : 21.9010141\n",
            "Iteration : 100 \t Cost : 21.9005751\n",
            "Iteration : 101 \t Cost : 21.9001698\n",
            "Iteration : 102 \t Cost : 21.8997957\n",
            "Iteration : 103 \t Cost : 21.8994501\n",
            "Iteration : 104 \t Cost : 21.8991309\n",
            "Iteration : 105 \t Cost : 21.8988359\n",
            "Iteration : 106 \t Cost : 21.8985632\n",
            "Iteration : 107 \t Cost : 21.8983109\n",
            "Iteration : 108 \t Cost : 21.8980776\n",
            "Iteration : 109 \t Cost : 21.8978617\n",
            "Iteration : 110 \t Cost : 21.8976619\n",
            "Iteration : 111 \t Cost : 21.8974768\n",
            "Iteration : 112 \t Cost : 21.8973053\n",
            "Iteration : 113 \t Cost : 21.8971464\n",
            "Iteration : 114 \t Cost : 21.8969990\n",
            "Iteration : 115 \t Cost : 21.8968624\n",
            "Iteration : 116 \t Cost : 21.8967356\n",
            "Iteration : 117 \t Cost : 21.8966179\n",
            "Iteration : 118 \t Cost : 21.8965086\n",
            "Iteration : 119 \t Cost : 21.8964071\n",
            "Iteration : 120 \t Cost : 21.8963127\n",
            "Iteration : 121 \t Cost : 21.8962249\n",
            "Iteration : 122 \t Cost : 21.8961433\n",
            "Iteration : 123 \t Cost : 21.8960673\n",
            "Iteration : 124 \t Cost : 21.8959965\n",
            "Iteration : 125 \t Cost : 21.8959306\n",
            "Iteration : 126 \t Cost : 21.8958692\n",
            "Iteration : 127 \t Cost : 21.8958119\n",
            "Iteration : 128 \t Cost : 21.8957584\n",
            "Iteration : 129 \t Cost : 21.8957085\n",
            "Iteration : 130 \t Cost : 21.8956618\n",
            "Iteration : 131 \t Cost : 21.8956183\n",
            "Iteration : 132 \t Cost : 21.8955775\n",
            "Iteration : 133 \t Cost : 21.8955394\n",
            "Iteration : 134 \t Cost : 21.8955037\n",
            "Iteration : 135 \t Cost : 21.8954702\n",
            "Iteration : 136 \t Cost : 21.8954389\n",
            "Iteration : 137 \t Cost : 21.8954095\n",
            "Iteration : 138 \t Cost : 21.8953819\n",
            "Iteration : 139 \t Cost : 21.8953559\n",
            "Iteration : 140 \t Cost : 21.8953316\n",
            "Iteration : 141 \t Cost : 21.8953087\n",
            "Iteration : 142 \t Cost : 21.8952871\n",
            "Iteration : 143 \t Cost : 21.8952668\n",
            "Iteration : 144 \t Cost : 21.8952477\n",
            "Iteration : 145 \t Cost : 21.8952297\n",
            "Iteration : 146 \t Cost : 21.8952127\n",
            "Iteration : 147 \t Cost : 21.8951966\n",
            "Iteration : 148 \t Cost : 21.8951815\n",
            "Iteration : 149 \t Cost : 21.8951671\n",
            "Iteration : 150 \t Cost : 21.8951535\n",
            "Iteration : 151 \t Cost : 21.8951407\n",
            "Iteration : 152 \t Cost : 21.8951285\n",
            "Iteration : 153 \t Cost : 21.8951170\n",
            "Iteration : 154 \t Cost : 21.8951060\n",
            "Iteration : 155 \t Cost : 21.8950956\n",
            "Iteration : 156 \t Cost : 21.8950857\n",
            "Iteration : 157 \t Cost : 21.8950763\n",
            "Iteration : 158 \t Cost : 21.8950674\n",
            "Iteration : 159 \t Cost : 21.8950589\n",
            "Iteration : 160 \t Cost : 21.8950508\n",
            "Iteration : 161 \t Cost : 21.8950431\n",
            "Iteration : 162 \t Cost : 21.8950357\n",
            "Iteration : 163 \t Cost : 21.8950287\n",
            "Iteration : 164 \t Cost : 21.8950219\n",
            "Iteration : 165 \t Cost : 21.8950155\n",
            "Iteration : 166 \t Cost : 21.8950094\n",
            "Iteration : 167 \t Cost : 21.8950035\n",
            "Iteration : 168 \t Cost : 21.8949979\n",
            "Iteration : 169 \t Cost : 21.8949925\n",
            "Iteration : 170 \t Cost : 21.8949873\n",
            "Iteration : 171 \t Cost : 21.8949823\n",
            "Iteration : 172 \t Cost : 21.8949776\n",
            "Iteration : 173 \t Cost : 21.8949730\n",
            "Iteration : 174 \t Cost : 21.8949686\n",
            "Iteration : 175 \t Cost : 21.8949644\n",
            "Iteration : 176 \t Cost : 21.8949603\n",
            "Iteration : 177 \t Cost : 21.8949564\n",
            "Iteration : 178 \t Cost : 21.8949527\n",
            "Iteration : 179 \t Cost : 21.8949491\n",
            "Iteration : 180 \t Cost : 21.8949456\n",
            "Iteration : 181 \t Cost : 21.8949422\n",
            "Iteration : 182 \t Cost : 21.8949389\n",
            "Iteration : 183 \t Cost : 21.8949358\n",
            "Iteration : 184 \t Cost : 21.8949328\n",
            "Iteration : 185 \t Cost : 21.8949299\n",
            "Iteration : 186 \t Cost : 21.8949271\n",
            "Iteration : 187 \t Cost : 21.8949243\n",
            "Iteration : 188 \t Cost : 21.8949217\n",
            "Iteration : 189 \t Cost : 21.8949191\n",
            "Iteration : 190 \t Cost : 21.8949167\n",
            "Iteration : 191 \t Cost : 21.8949143\n",
            "Iteration : 192 \t Cost : 21.8949120\n",
            "Iteration : 193 \t Cost : 21.8949098\n",
            "Iteration : 194 \t Cost : 21.8949076\n",
            "Iteration : 195 \t Cost : 21.8949055\n",
            "Iteration : 196 \t Cost : 21.8949035\n",
            "Iteration : 197 \t Cost : 21.8949015\n",
            "Iteration : 198 \t Cost : 21.8948996\n",
            "Iteration : 199 \t Cost : 21.8948977\n",
            "Iteration : 200 \t Cost : 21.8948959\n",
            "Iteration : 201 \t Cost : 21.8948942\n",
            "Iteration : 202 \t Cost : 21.8948925\n",
            "Iteration : 203 \t Cost : 21.8948909\n",
            "Iteration : 204 \t Cost : 21.8948893\n",
            "Iteration : 205 \t Cost : 21.8948877\n",
            "Iteration : 206 \t Cost : 21.8948862\n",
            "Iteration : 207 \t Cost : 21.8948848\n",
            "Iteration : 208 \t Cost : 21.8948833\n",
            "Iteration : 209 \t Cost : 21.8948820\n",
            "Iteration : 210 \t Cost : 21.8948806\n",
            "Iteration : 211 \t Cost : 21.8948793\n",
            "Iteration : 212 \t Cost : 21.8948781\n",
            "Iteration : 213 \t Cost : 21.8948768\n",
            "Iteration : 214 \t Cost : 21.8948756\n",
            "Iteration : 215 \t Cost : 21.8948745\n",
            "Iteration : 216 \t Cost : 21.8948734\n",
            "Iteration : 217 \t Cost : 21.8948723\n",
            "Iteration : 218 \t Cost : 21.8948712\n",
            "Iteration : 219 \t Cost : 21.8948702\n",
            "Iteration : 220 \t Cost : 21.8948691\n",
            "Iteration : 221 \t Cost : 21.8948682\n",
            "Iteration : 222 \t Cost : 21.8948672\n",
            "Iteration : 223 \t Cost : 21.8948663\n",
            "Iteration : 224 \t Cost : 21.8948654\n",
            "Iteration : 225 \t Cost : 21.8948645\n",
            "Iteration : 226 \t Cost : 21.8948636\n",
            "Iteration : 227 \t Cost : 21.8948628\n",
            "Iteration : 228 \t Cost : 21.8948620\n",
            "Iteration : 229 \t Cost : 21.8948612\n",
            "Iteration : 230 \t Cost : 21.8948604\n",
            "Iteration : 231 \t Cost : 21.8948597\n",
            "Iteration : 232 \t Cost : 21.8948589\n",
            "Iteration : 233 \t Cost : 21.8948582\n",
            "Iteration : 234 \t Cost : 21.8948575\n",
            "Iteration : 235 \t Cost : 21.8948569\n",
            "Iteration : 236 \t Cost : 21.8948562\n",
            "Iteration : 237 \t Cost : 21.8948556\n",
            "Iteration : 238 \t Cost : 21.8948549\n",
            "Iteration : 239 \t Cost : 21.8948543\n",
            "Iteration : 240 \t Cost : 21.8948537\n",
            "Iteration : 241 \t Cost : 21.8948532\n",
            "Iteration : 242 \t Cost : 21.8948526\n",
            "Iteration : 243 \t Cost : 21.8948521\n",
            "Iteration : 244 \t Cost : 21.8948515\n",
            "Iteration : 245 \t Cost : 21.8948510\n",
            "Iteration : 246 \t Cost : 21.8948505\n",
            "Iteration : 247 \t Cost : 21.8948500\n",
            "Iteration : 248 \t Cost : 21.8948495\n",
            "Iteration : 249 \t Cost : 21.8948491\n",
            "Iteration : 250 \t Cost : 21.8948486\n",
            "R^2: 0.7406424628192569\n",
            "Adjusted R^2 : 0.7602122839913105\n",
            "F-statistic:  117.32090577194035\n",
            "\n",
            "using normal equation: \n",
            "\n",
            "Cost: 21.894831181729206\n",
            "R^2: 0.7406426641094094\n",
            "Adjusted R^2 : 0.7602124906001052\n",
            "F-statistic:  117.32102871125619\n",
            "scikit training model score:  0.7406426641094095\n",
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                      y   R-squared (uncentered):                   0.959\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.958\n",
            "Method:                 Least Squares   F-statistic:                              891.3\n",
            "Date:                Tue, 24 Aug 2021   Prob (F-statistic):                        0.00\n",
            "Time:                        15:45:33   Log-Likelihood:                         -1523.8\n",
            "No. Observations:                 506   AIC:                                      3074.\n",
            "Df Residuals:                     493   BIC:                                      3128.\n",
            "Df Model:                          13                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "x1            -0.0929      0.034     -2.699      0.007      -0.161      -0.025\n",
            "x2             0.0487      0.014      3.382      0.001       0.020       0.077\n",
            "x3            -0.0041      0.064     -0.063      0.950      -0.131       0.123\n",
            "x4             2.8540      0.904      3.157      0.002       1.078       4.630\n",
            "x5            -2.8684      3.359     -0.854      0.394      -9.468       3.731\n",
            "x6             5.9281      0.309     19.178      0.000       5.321       6.535\n",
            "x7            -0.0073      0.014     -0.526      0.599      -0.034       0.020\n",
            "x8            -0.9685      0.196     -4.951      0.000      -1.353      -0.584\n",
            "x9             0.1712      0.067      2.564      0.011       0.040       0.302\n",
            "x10           -0.0094      0.004     -2.395      0.017      -0.017      -0.002\n",
            "x11           -0.3922      0.110     -3.570      0.000      -0.608      -0.176\n",
            "x12            0.0149      0.003      5.528      0.000       0.010       0.020\n",
            "x13           -0.4163      0.051     -8.197      0.000      -0.516      -0.317\n",
            "==============================================================================\n",
            "Omnibus:                      204.082   Durbin-Watson:                   0.999\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1374.225\n",
            "Skew:                           1.609   Prob(JB):                    3.90e-299\n",
            "Kurtosis:                      10.404   Cond. No.                     8.50e+03\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 8.5e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}